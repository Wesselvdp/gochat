# Download
scp root@142.93.224.213:/etc/dokploy/compose/gochat-app-staging-bd0bf7/files/sqlite/database.db ./local_db.sqlite
# Upload
scp ./local_db.sqlite root@142.93.224.213:/etc/dokploy/compose/gochat-app-staging-bd0bf7/files/sqlite/new_database.db
# Remote backup
ssh root@142.93.224.213 "cp /etc/dokploy/compose/gochat-app-staging-bd0bf7/files/sqlite/database.db /etc/dokploy/compose/gochat-app-staging-bd0bf7/files/sqlite/database.db.backup"

curl https://cqzqss48xndt4b-11434.proxy.runpod.net/api/generate -d '{
    "model": "gemma2:27b",
    "prompt": "Why is the sky blue?"
}'

Attu
docker run -p 8000:3000 -e MILVUS_URL=142.93.224.213:19530 zilliz/attu:v2.4

sudo chown -R torgon:torgon /etc/dokploy

python -m vllm.entrypoints.openai.api_server --model google/gemma-2-27b --dtype bfloat16 --gpu-memory-utilization 0.9 --quantization bitsandbytes --max-model-len 1024
python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Small-3.1-24B-Instruct-2503 \
python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-Small-3.1-24B-Instruct-2503 \
  --config_format mistral \
  --load_format mistral \
  --tool-call-parser mistral \
  --tokenizer-mode mistral \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.9


  python -m vllm.entrypoints.openai.api_server --model stelterlab/Mistral-Small-24B-Instruct-2501-AWQ \
  --config_format mistral \
  --load_format mistral \
  --tool-call-parser mistral \
  --tokenizer-mode mistral \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.9 \


   vllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 \
    --config_format mistral \
    --load_format mistral \
    --tool-call-parser mistral \
    --tokenizer-mode mistral \
    --dtype auto \
    --gpu-memory-utilization 0.95 \
    --quantization bitsandbytes \
     --cpu-offload-gb 15 \
    --max-model-len 12000 \

python -m vllm.entrypoints.openai.api_server \
    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \
    --dtype bfloat16 \
    --gpu-memory-utilization 0.9 \
    --tensor-parallel-size 1 \
